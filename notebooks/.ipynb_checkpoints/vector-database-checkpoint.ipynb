{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Required Libraries\n",
    "Make sure the necessary libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: pgvector in /opt/conda/lib/python3.11/site-packages (0.3.5)\n",
      "Requirement already satisfied: psycopg in /opt/conda/lib/python3.11/site-packages (3.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from pgvector) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from psycopg) (4.12.2)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.51.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.8/791.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.9.11 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install python-dotenv\n",
    "!pip install pgvector psycopg\n",
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Set up REST API interaction\n",
    "In this step, we will define a function that interacts with the Flask API to process documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def process_document_via_api(file_path):\n",
    "    \"\"\"\n",
    "    This function sends a request to the document processor API to process the document.\n",
    "    It sends the file path as a payload to the API.\n",
    "    \"\"\"\n",
    "    # Define the API URL (adjust if running on a different host)\n",
    "    api_url = \"http://doc_processor:5000/process_document\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    # if not os.path.exists(file_path):\n",
    "    #     raise ValueError(f\"The file at {file_path} does not exist.\")\n",
    "\n",
    "    # Create the payload with the file path\n",
    "    payload = {\"file_path\": file_path}\n",
    "\n",
    "    # Send the POST request to the API\n",
    "    response = requests.post(api_url, json=payload)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Document processed successfully.\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response)\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Process a Document via the REST API\n",
    "Provide the file path to the document you want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processed successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Document processed successfully. Number of chunks: 47'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example file path (adjust this to point to your document)\n",
    "file_path = \"/app/documents/carbon-free-energy.pdf\"\n",
    "\n",
    "# Call the REST API to process the document\n",
    "response = process_document_via_api(file_path)\n",
    "\n",
    "# Output the API response\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform Semantic Search Query (Optional)\n",
    "Once the document is processed, you can modify this step to interact with the database for querying embeddings.\n",
    "Here, you can build additional functionality to run queries directly on the database using `psycopg2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a description to search:  Hi, what are Google plans for the future?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: documents\n",
      "Document: SEPTEMBER 2020 24/7 BY 2030: REALIZING A CARBON-FREE FUTUREdaunting challenge, it’s also an epic opportunity—a once-in-history \n",
      "chance to fundamentally reshape the world’s energy systems for  \n",
      "the better.\n",
      "Google will continue to lead the way toward a clean energy future in \n",
      "our own operations, but to create broader change we need your \n",
      "help. Let’s work together and make a carbon-free economy a reality, \n",
      "this decade. The planet can’t wait any longer. \n",
      "Notes\n",
      "1. To ensure that Google is the driver for bringing new clean energy onto the grid, we \n",
      "insist that all projects we buy electricity from be “ additional .” This means that we \n",
      "seek to purchase energy from not yet constructed generation facilities that will be \n",
      "built above and beyond what’s required by existing energy regulations.\n",
      "2. Google remains unwavering in our commitment to the United Nations Framework \n",
      "Convention on Climate Change’s 2015 Paris Agreement , which targets aggressive\n",
      "Metadata: {'source': '/app/documents/carbon-free-energy.pdf', 'page': 16}\n",
      "---------\n",
      "Collection: documents\n",
      "Document: SEPTEMBER 2020 24/7 BY 2030: REALIZING A CARBON-FREE FUTUREdaunting challenge, it’s also an epic opportunity—a once-in-history \n",
      "chance to fundamentally reshape the world’s energy systems for  \n",
      "the better.\n",
      "Google will continue to lead the way toward a clean energy future in \n",
      "our own operations, but to create broader change we need your \n",
      "help. Let’s work together and make a carbon-free economy a reality, \n",
      "this decade. The planet can’t wait any longer. \n",
      "Notes\n",
      "1. To ensure that Google is the driver for bringing new clean energy onto the grid, we \n",
      "insist that all projects we buy electricity from be “ additional .” This means that we \n",
      "seek to purchase energy from not yet constructed generation facilities that will be \n",
      "built above and beyond what’s required by existing energy regulations.\n",
      "2. Google remains unwavering in our commitment to the United Nations Framework \n",
      "Convention on Climate Change’s 2015 Paris Agreement , which targets aggressive\n",
      "Metadata: {'source': '/app/documents/carbon-free-energy.pdf', 'page': 16}\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import os\n",
    "import numpy as np\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model (assuming you are using OpenAI embeddings)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "conn = psycopg.connect(\n",
    "    dbname=os.getenv(\"POSTGRES_DB\"),\n",
    "    host=os.getenv(\"POSTGRES_HOST\"),\n",
    "    user=os.getenv(\"POSTGRES_USER\"),\n",
    "    password=os.getenv(\"POSTGRES_PASSWORD\")\n",
    ")\n",
    "\n",
    "# Register the vector extension in PostgreSQL\n",
    "register_vector(conn)\n",
    "\n",
    "# Function to query the database for similar documents\n",
    "def get_top_similar_documents(user_input: str):\n",
    "    # Embed the user input using embed_query for single queries\n",
    "    query_vector = embedding_model.embed_query(user_input)\n",
    "\n",
    "    # Convert the query_vector to a format that PostgreSQL expects\n",
    "    query_vector_str = '[' + ','.join(map(str, query_vector)) + ']'\n",
    "\n",
    "    # Query the vector database for the top 2 similar documents\n",
    "    response = conn.execute(\n",
    "        '''\n",
    "        SELECT lc.name AS collection_name, le.document, le.cmetadata \n",
    "        FROM langchain_pg_embedding le\n",
    "        JOIN langchain_pg_collection lc ON le.collection_id = lc.uuid\n",
    "        ORDER BY le.embedding <-> %s::vector LIMIT 2\n",
    "        ''',\n",
    "        (query_vector_str,)\n",
    "    ).fetchall()\n",
    "\n",
    "    # Display the results\n",
    "    for hit in response:\n",
    "        # Accessing the values by their index in the tuple\n",
    "        print(f\"Collection: {hit[0]}\")  # collection_name\n",
    "        print(f\"Document: {hit[1]}\")    # document\n",
    "        print(f\"Metadata: {hit[2]}\")    # cmetadata\n",
    "        print(\"---------\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a description to search: \")\n",
    "get_top_similar_documents(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
